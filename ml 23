#1
It may lead to some amount of data loss.
PCA tends to find linear correlations between variables, which is sometimes undesirable.
PCA fails in cases where mean and covariance are not enough to define datasets.
PCA cannot be used for Naive Bias classification as it tries to establish relationship between features

#2
The curse of dimensionality basically means that the error increases with the increase in the number of features. It refers to the fact that algorithms are harder to design in high dimensions and often have a running time exponential in the dimensions.

#3
In order to be able to reconstruct the original two variables from this one principal component, we can map it back to p dimensions with V‚ä§.
#5
 If I perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. In this case roughly 950 dimensions are required to preserve 95% of the variance. So the answer is, it depends on the dataset, and it could be any number between 1 and 950.This could be determined by Explained varience ratio and plotting screen plot
 
 #6
 The following are the scenarios where the following are used:

Vanilla PCA: the dataset fit in memory
Incremental PCA: larget dataset that don't fit in memory, online taks
Randomized PCA: considerably reduce dimensionality and the dataset fit the memory.
kernel PCA: used for nonlinear PCA

#7
 By doing PCA, it is a good choice for dimensionality reduction and visualization for datasets with a large number of variables.Reduces model complexity.
 
 #8
  it often make any sense to chain two different dimensionality reduction algorithms. A common example is using PCA to quickly get rid of a large number of useless dimensions, then applying another much slower dimensionality reduction algorithm, such as LLE.
