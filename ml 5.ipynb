{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "Get Data: The first step in the Machine Learning process is getting data.\n",
    "Cleaning, Preparing & Manipulating Data: Real-world data often has unorganized, missing, or noisy elements.\n",
    "Train Model: This step is where the magic happens!\n",
    "Testing Model.\n",
    "Improving model.\n",
    "Data preprocessing involves transforming raw data to well-formed data sets so that data mining analytics can be applied.\n",
    "Preprocessing involves both data validation and data imputation\n",
    "The Goal of Data Validation is to assess whether the data in question is both complete and accurate.\n",
    "The Goal of Data Imputation is to correct errors and input missing values, Either Manually or Automatically through business process automation (BPA) programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "Quantitative Data Type: This Type Of Data Type Consists Of Numerical Values. Anything Which Is Measured By Numbers\n",
    "    They are of 2 types: Contious and decrete\n",
    "Qualitative Data Type: These Are The Data Types That Cannot Be Expressed In Numbers. This Describes Categories Or Groups And Is Hence Known As The Categorical Data Type.\n",
    "    They areof 2 types: Ordinal,Nominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    " Noisy data, dirty data, and incomplete data are the quintessential enemies of ideal Machine Learning. The solution to this conundrum is to take the time to evaluate and scope data with meticulous data governance, data integration, and data exploration until you get clear data. Ramifications or major issues in machine learning are:\n",
    "\n",
    "Five practical issues in machine learning and the business implications Data quality.\n",
    "Machine learning systems rely on data.\n",
    "The complexity and quality trade-off.\n",
    "Sampling bias in data.\n",
    "Changing expectations and concept drift.\n",
    "Monitoring and maintenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    " Even in a Well-Designed & Controlled study, Missing data occurs in almost all research. Missing data can reduce the statistical power of a study and can produce biased estimates, leading to invalid conclusions.\n",
    "\n",
    "Real-world data collection has its own set of problems, It is often very messy which includes missing data, presence of outliers, unstructured manner, etc.\n",
    "Before looking for any insights from the data, we have to first perform preprocessing tasks which then only allow us to use that data for further observation and train our machine learning model.\n",
    "Missing value in a dataset is a very common phenomenon in the reality.\n",
    "Missing value correction is required to reduce bias and to produce powerful suitable models.\n",
    "Most of the algorithms can’t handle missing data, thus you need to act in some way to simply not let your code crash. So, let’s begin with the methods to solve the problem..Varioues techniques are imputaion,deleting those records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "Delete the observations: If there is a large number of observations in the dataset, where all the classes to be predicted are sufficiently represented in the training data, then try deleting the missing value observations, which would not bring significant change in your feed to your model. For Example Implement this method in a given dataset, we can delete the entire row which contains missing values.\n",
    "Replace missing values with the most frequent value: You can always impute them based on Mode in the case of categorical variables, just make sure you don’t have highly skewed class distributions.\n",
    "Develop a model to predict missing values: One smart way of doing this could be training a classifier over your columns with missing values as a dependent variable against other features of your data set and trying to impute based on the newly trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "Data Cleaning: The data can have many irrelevant and missing parts. To handle this part, data cleaning is done. It involves handling of missing data, noisy data etc.\n",
    "Missing Data: This situation arises when some data is missing in the data. It can be handled in various ways. Some of them are:\n",
    "Ignore the tuples: This approach is suitable only when the dataset we have is quite large and multiple values are missing within a tuple.\n",
    "Fill the Missing values: There are various ways to do this task. You can choose to fill the missing values manually, by attribute mean or the most probable value.\n",
    "Noisy Data: Noisy data is a meaningless data that can’t be interpreted by machines.It can be generated due to faulty data collection, data entry errors etc. It can be handled in following ways :\n",
    "Binning Method: This method works on sorted data in order to smooth it. The whole data is divided into segments of equal size and then various methods are performed to complete the task. Each segmented is handled separately. One can replace all data in a segment by its mean or boundary values can be used to complete the task.\n",
    "\n",
    "Dimensionality Reduction: This reduce the size of data by encoding mechanisms.It can be lossy or lossless. If after reconstruction from compressed data, original data can be retrieved, such reduction are called lossless reduction else it is called lossy reduction. The two effective methods of dimensionality reduction are:Wavelet transforms and PCA (Principal Component Analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9a\n",
    "Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.\n",
    "Q3 is the third quartile of the data, i.e., to say 75% of the data lies between minimum and Q3.\n",
    "The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9b\n",
    "minimum is the minimum value in the dataset\n",
    "maximum is the maximum value in the dataset.\n",
    "So the difference between the two tells us about the range of dataset.\n",
    "The median is the median (or centre point), also called second quartile, of the data (resulting from the fact that the data is ordered).\n",
    "Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.\n",
    "Q3 is the third quartile of the data, i.e., to say 75%\n",
    "When the data is left skewed, lower whisker will be longer than upper whisker.\n",
    "To detect the outliers this method is used, we define a new range, let’s call it decision range, and any data point lying outside this range is considered as outlier and is accordingly dealt with. The range is as given below: Lower Bound: (Q1 - 1.5 * IQR)Upper Bound: (Q3 + 1.5 * IQR)\n",
    "The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10a\n",
    "Data collected at regular intervals:\n",
    "Interval data is one of the two types of discrete data.\n",
    "An example of interval data is the data collected on a thermometer—its gradation or markings are equidistant.\n",
    "Unlike ordinal data, interval data always take numerical values where the distance between two points on the scale is standardised and equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10b\n",
    "The gap between the quartiles:\n",
    "Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.\n",
    "Q3 is the third quartile of the data, i.e., to say 75% of the data lies between minimum and Q3.\n",
    "The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11a\n",
    "The average and median:\n",
    "The mean (informally, the “average“) is found by adding all of the numbers together and dividing by the number of items in the set: 10 + 10 + 20 + 40 + 70 / 5 = 30. The median is found by ordering the set from lowest to highest and finding the exact middle. The median is just the middle number: 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11b\n",
    "Histogram and barplot:\n",
    "Histograms and box plots are very similar in that they both help to visualize and describe numeric data. Although histograms are better in determining the underlying distribution of the data, box plots allow you to compare multiple data sets better than histograms as they are less detailed and take up less space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
